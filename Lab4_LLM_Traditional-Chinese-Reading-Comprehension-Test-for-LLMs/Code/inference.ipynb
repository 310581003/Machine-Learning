{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ObTKtqY0yEh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path and version numbers for python depend on operating system\n",
        "!which python\n",
        "!python --version"
      ],
      "metadata": {
        "id": "9l8CfPcHBpo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# environment variable\n",
        "%env PYTHONPATH="
      ],
      "metadata": {
        "id": "Ru9KFq-5C9mQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# install virtual environment package\n",
        "!pip install virtualenv"
      ],
      "metadata": {
        "id": "JcOOKuTTGiEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create virtual environment\n",
        "!virtualenv llm"
      ],
      "metadata": {
        "id": "qfjBCjArGm7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh"
      ],
      "metadata": {
        "id": "e42X4HfGGsLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod +x Miniconda3-latest-Linux-x86_64.sh"
      ],
      "metadata": {
        "id": "wN-pCv2ZGwUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local"
      ],
      "metadata": {
        "id": "1Gmu4bBNGy56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chardet"
      ],
      "metadata": {
        "id": "vDOe9YUFHRfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!conda install -q -y --prefix /usr/local python=3.10 ujson"
      ],
      "metadata": {
        "id": "YTEfrXrmG2RK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/usr/local/lib/python3.10/site-packages/')"
      ],
      "metadata": {
        "id": "hbvfdn_eEGP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# activate conda enviornment\n",
        "import os\n",
        "os.environ['CONDA_PREFIX'] = '/usr/local/envs/llm'"
      ],
      "metadata": {
        "id": "XD2oGxB_DQX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/ColabNotebooks/2023ML/Lab4/Chinese-LLaMA-Alpaca-2-main/Chinese-LLaMA-Alpaca-2-main\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "jVQGjfcUX2UH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())\n",
        "torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "rXuW2uhrX7I2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "id": "56WoFWsgoMxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "bjw7wxkaX9M0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install deepspeed"
      ],
      "metadata": {
        "id": "A4wjueazYDGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scipy"
      ],
      "metadata": {
        "id": "nh7o6wGhYE0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate"
      ],
      "metadata": {
        "id": "QZjfU4HBYGUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydantic==1.10.9"
      ],
      "metadata": {
        "id": "yA8KM4DpIt3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 以下範例為輸入json用以批次生成的腳本，並將結果存於 ./answer.csv\n",
        "# 你也可以自行修改腳本，整理成自己習慣的資料格式(txt..)用於批次輸入\n",
        "import torch\n",
        "import os\n",
        "import argparse\n",
        "import json,csv\n",
        "from transformers import (\n",
        "    LlamaForCausalLM,\n",
        "    LlamaTokenizer,\n",
        "    AutoTokenizer,\n",
        "    StoppingCriteria,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "\n",
        "# 訓練時System Prompt可以自訂，生成時建議與訓練的Prompt一致\n",
        "# 請參考script/training/build_dataset.py 進行Prompt的調整\n",
        "DEFAULT_SYSTEM_PROMPT = \"\"\"你是一個樂於助人的助手。\"\"\"\n",
        "\n",
        "TEMPLATE_WITH_SYSTEM_PROMPT = (\n",
        "    \"[INST] <<SYS>>\\n\"\n",
        "    \"{system_prompt}\\n\"\n",
        "    \"<</SYS>>\\n\\n{instruction} [/INST]\"\n",
        ")\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\n",
        "    '--base_model',\n",
        "    default='/content/drive/MyDrive/ColabNotebooks/2023ML/Lab4/Chinese-LLaMA-Alpaca-2-main/Chinese-LLaMA-Alpaca-2-main/llama-2-13b-combined-test',\n",
        "    type=str,\n",
        "    #required=True,\n",
        "    help='Base model path')\n",
        "parser.add_argument(\n",
        "    '--gpus',\n",
        "    default=\"0\",\n",
        "    type=str,\n",
        "    help='If None, cuda:0 will be used. Inference using multi-cards: --gpus=0,1,... ')\n",
        "parser.add_argument(\n",
        "    '--load_in_8bit',\n",
        "    action='store_true',\n",
        "    help='Use 8 bit quantized model')\n",
        "parser.add_argument(\n",
        "    '--load_in_4bit',\n",
        "    action='store_true',\n",
        "    help='Use 4 bit quantized model')\n",
        "args = parser.parse_args(args=[])\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpus\n",
        "\n",
        "# Get GPU devices\n",
        "DEV = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load Model\n",
        "model = LlamaForCausalLM.from_pretrained(\n",
        "    args.base_model,\n",
        "    torch_dtype=torch.float16,\n",
        "    #low_cpu_mem_usage=True,\n",
        "    #device_map='auto',\n",
        "    #quantization_config=BitsAndBytesConfig(\n",
        "        #load_in_4bit=args.load_in_4bit,\n",
        "        #load_in_8bit=args.load_in_8bit,\n",
        "        #bnb_4bit_compute_dtype=torch.float16\n",
        "    #)\n",
        ")\n",
        "\n",
        "# Load Tokenizer\n",
        "tokenizer = LlamaTokenizer.from_pretrained(args.base_model)\n",
        "\n",
        "# Do inference\n",
        "with open('/content/drive/MyDrive/ColabNotebooks/2023ML/Lab4/AI1000.json', 'r') as file:\n",
        "    json_data = json.load(file)\n",
        "    with open('/content/drive/MyDrive/ColabNotebooks/2023ML/Lab4/answer.csv', 'w', newline='', encoding='utf-8') as csv_file:\n",
        "        writer=csv.writer(csv_file)\n",
        "        writer.writerow(['ID', 'Answer'])\n",
        "        idx=1\n",
        "        for row in json_data:\n",
        "            id = idx\n",
        "            instruction = row['instruction'] + '\\n' + row['input']\n",
        "\n",
        "            prompt = TEMPLATE_WITH_SYSTEM_PROMPT.format_map({'instruction': instruction,'system_prompt': DEFAULT_SYSTEM_PROMPT})\n",
        "            inputs = tokenizer.encode(prompt+'\\n', return_tensors=\"pt\").to(DEV)\n",
        "\n",
        "            generate_kwargs = dict(\n",
        "                input_ids=inputs,\n",
        "                temperature=0.2,\n",
        "                top_p=0.9,\n",
        "                top_k=40,\n",
        "                do_sample=True,\n",
        "                max_new_tokens=1, #為了回答選擇題而設定1\n",
        "                repetition_penalty=1.1,\n",
        "                guidance_scale=1.0\n",
        "            )\n",
        "            outputs = model.to(DEV).generate(**generate_kwargs)\n",
        "            result = tokenizer.decode(outputs[0])\n",
        "            print(result)\n",
        "            response = result.split('[/INST]\\n')[-1]\n",
        "            writer.writerow([id, response[0]])\n",
        "            idx=idx+1"
      ],
      "metadata": {
        "id": "Vd8j07zMYPV_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}