{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyPe4p1lo/mdhsN+mLSz+nrB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"-jHn-XfHvMeX"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["from __future__ import unicode_literals, print_function, division\n","import torch\n","import sys\n","sys.path.append('/content/drive/MyDrive/Colab Notebooks/2023ML/Lab3')\n","print(torch.__version__)\n","print(torch.cuda.is_available())\n","\n","import os\n","from torch.utils.data import Dataset, DataLoader, TensorDataset, RandomSampler\n","import torchvision.transforms as T\n","from torchvision import models\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchvision.models.detection as detection\n","\n","import copy\n","\n","from PIL import Image\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import argparse\n","import csv\n","from pathlib import Path\n","from torch.utils.data import random_split\n","from tqdm import tqdm\n","\n","torch.backends.cudnn.benchmark = True\n","torch.cuda.get_device_name(0)\n","from io import open\n","import unicodedata\n","import re\n","import random\n","import time\n","import math\n","import matplotlib.ticker as ticker\n","from nltk import word_tokenize\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","import jieba\n","import spacy\n","nlp = spacy.load(\"en_core_web_sm\")"],"metadata":{"id":"sjrPb90YvpTs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!unzip '/content/drive/MyDrive/Colab Notebooks/2023ML/Lab3/Data/train-ZH.csv.zip' -d '/content'"],"metadata":{"id":"ysWNAnqa5pWi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!unzip '/content/drive/MyDrive/Colab Notebooks/2023ML/Lab3/Data/train-TL.csv.zip' -d '/content'"],"metadata":{"id":"W3EUdNAU52NB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["###########################################################################\n","##                      Argparse Setting                                 ##\n","###########################################################################\n","def parse_config():\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument(\"--train_data\", default='/content/train-ZH.csv', help=\"path of training data\")\n","    parser.add_argument(\"--train_target\", default='/content/train-TL.csv', help=\"path of training target\")\n","    parser.add_argument(\"--test_data\", default='/content/drive/MyDrive/Colab Notebooks/2023ML/Lab3/Data/test-ZH-nospace.csv', help=\"path of testing data\")\n","\n","    parser.add_argument(\"--n_epochs\", type=int, default=100, help=\"number of epochs of training\")\n","    parser.add_argument(\"--batch_size\", type=int, default=700, help=\"size of the batches\")\n","    parser.add_argument(\"--hidden_size\", type=int, default=512, help=\"hidden size of RNN\")\n","    parser.add_argument(\"--lr\", type=float, default=0.0001, help=\"adam: learning rate\")\n","    parser.add_argument(\"--opt\",  default='Adam', help=\"Optimizer:'SGD','RMSprop','Adagrad','Adam'\")\n","    parser.add_argument(\"--b1\", type=float, default=0.9, help=\"adam: decay of first order momentum of gradient\")\n","    parser.add_argument(\"--b2\", type=float, default=0.97, help=\"adam: decay of second order momentum of gradient\")\n","    parser.add_argument(\"--loss\", default='Cross_entropy', help=\"Loss func:'hinge_loss','MAE','MSE','NLL(Negative Log-Likelihood)','Cross_entropy'\")\n","    parser.add_argument(\"--l2_reg\", type=float, default=0.0006, help=\"l2 regularization\")\n","    parser.add_argument(\"--val_split\", type=float, default=0.2, help=\"validation split\")\n","\n","    parser.add_argument('--outf', default='/content/drive/MyDrive/Colab Notebooks/2023ML/Lab3/Result/', help='folder to output record and model')\n","    parser.add_argument('--log', default='/content/drive/MyDrive/Colab Notebooks/2023ML/Lab3/Result/record.txt', help='path to record')\n","    parser.add_argument('--font', default=20, help='font size for output figure')\n","\n","    args = parser.parse_args(args=[])\n","    return args\n"],"metadata":{"id":"4tcP2zHwv6WK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["###########################################################################\n","##                        Preparing Data                                 ##\n","###########################################################################\n","SOS_token = 0\n","EOS_token = 1\n","MAX_LENGTH = 60\n","args = parse_config()\n","class Lang:\n","    def __init__(self, name):\n","        self.name = name\n","        self.word2index = {}\n","        self.word2count = {}\n","        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n","        self.n_words = 2  # Count SOS and EOS\n","\n","    def addSentence(self, sentence):\n","        if self.name == 'talo':\n","            for word in nlp(sentence):\n","                self.addWord(str(word))\n","        else:\n","            for word in sentence.replace(\" \", \"\"):\n","                self.addWord(word)\n","\n","\n","    def addWord(self, word):\n","        if word not in self.word2index:\n","            self.word2index[word] = self.n_words\n","            self.word2count[word] = 1\n","            self.index2word[self.n_words] = word\n","            self.n_words += 1\n","        else:\n","            self.word2count[word] += 1\n","\n","\n","def readLangs(lang1, lang2, reverse=False):\n","    print(\"Reading lines...\")\n","\n","    # Read the file and split into lines\n","    lines_lang1 = pd.read_csv(args.train_data, usecols=[\"txt\"], encoding='utf-8').values.tolist()\n","    lines_lang2 = pd.read_csv(args.train_target, usecols=[\"txt\"], encoding='utf-8').values.tolist()\n","\n","    # Reverse pairs, make Lang instances\n","    input_lang = Lang(lang1)\n","    output_lang = Lang(lang2)\n","\n","    return input_lang, output_lang, lines_lang1, lines_lang2\n","\n","\n","def filterPair(p):\n","    return len(p.split(' ')) < MAX_LENGTH\n","        #len(p[1].split(' ')) < MAX_LENGTH\n","        #p[1].startswith(eng_prefixes)\n","\n","\n","def filterPairs(pairs):\n","    return [pair for pair in pairs if filterPair(pair)]\n","\n","def prepareData(lang1, lang2, reverse=False):\n","    input_lang, output_lang, lines_lang1, lines_lang2 = readLangs(lang1, lang2, reverse)\n","    print(\"Read %s sentences of input_lang\" % len(lines_lang1))\n","    print(\"Read %s sentences of output_lang\" % len(lines_lang2))\n","    with open(args.log, 'a') as f:\n","        f.write(f'------------------------------------------\\n')\n","        f.write(f'-     Data pre-processing     -\\n')\n","        f.write(f'------------------------------------------\\n')\n","        f.write(\">>> Reading data ...\\n\")\n","        f.write(\"Read %s sentences of input_lang (Chinese)\\n\" % len(lines_lang1))\n","        f.write(\"Read %s sentences of output_lang (Tailo)\\n\" % len(lines_lang2))\n","\n","    print(\"Counting words...\")\n","    for pair in lines_lang1:\n","        input_lang.addSentence(pair[0])\n","\n","    lines_lang1_test = pd.read_csv(args.test_data, usecols=[\"txt\"], encoding='utf-8').values.tolist()\n","    for pair in lines_lang1_test:\n","        input_lang.addSentence(pair[0])\n","\n","    for pair in lines_lang2:\n","        output_lang.addSentence(pair[0])\n","    print(\"Counted words:\")\n","    print(input_lang.name, input_lang.n_words)\n","    print(output_lang.name, output_lang.n_words)\n","    with open(args.log, 'a') as f:\n","            f.write(\"- Counted words :\\n\")\n","            f.write(f'Chinese : {input_lang.n_words} words in total\\n')\n","            f.write(f'Tailo : {output_lang.n_words} words in total\\n')\n","    return input_lang, output_lang, lines_lang1, lines_lang2\n","\n","\n","def indexesFromSentence(lang, sentence):\n","    if lang.name == 'talo':\n","        return [lang.word2index[str(word)] for word in nlp(sentence)]\n","    else :\n","\n","        return [lang.word2index[word] for word in sentence.replace(\" \", \"\")]\n","\n","def tensorFromSentence(lang, sentence):\n","\n","    indexes = indexesFromSentence(lang, sentence)\n","    indexes.append(EOS_token)\n","    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n","\n","def tensorsFromPair(pair):\n","    input_tensor = tensorFromSentence(input_lang, pair[0])\n","    target_tensor = tensorFromSentence(output_lang, pair[1])\n","    return (input_tensor, target_tensor)\n","\n","\n","def get_dataloader():\n","    args = parse_config()\n","    input_lang, output_lang, lines_lang1, lines_lang2 = prepareData('ch', 'talo', True)\n","\n","    n = len(lines_lang1)\n","    print(f'length of lang1 : {len(lines_lang1)}')\n","    print(f'length of lang2 : {len(lines_lang2)}')\n","    assert len(lines_lang1)==len(lines_lang2)\n","    input_ids = np.ones((n, MAX_LENGTH), dtype=np.int32)\n","    target_ids = np.ones((n, MAX_LENGTH), dtype=np.int32)\n","\n","    idx = 0\n","    for sent in lines_lang1:\n","        inp_ids = indexesFromSentence(input_lang, sent[0])\n","        inp_ids.append(EOS_token)\n","\n","        input_ids[idx, :len(inp_ids)] = inp_ids\n","        idx = idx+1\n","\n","    idx = 0\n","    for sent in lines_lang2:\n","        tgt_ids = indexesFromSentence(output_lang, sent[0])\n","        tgt_ids.append(EOS_token)\n","\n","        target_ids[idx, :len(tgt_ids)] = tgt_ids\n","        idx = idx+1\n","\n","    train_data = TensorDataset(torch.LongTensor(input_ids).to(device),\n","                               torch.LongTensor(target_ids).to(device))\n","\n","    train_size = int(0.8 * len(train_data))\n","    valid_size = len(train_data) - train_size\n","    train_dataset, valid_dataset = random_split(train_data, [train_size, valid_size])\n","    train_loader = DataLoader(dataset=train_dataset, batch_size=args.batch_size, shuffle=True)\n","    valid_loader= DataLoader(dataset=valid_dataset, batch_size=args.batch_size, shuffle=True)\n","    print(f'word2index of ch : {input_lang.word2index}')\n","    print(f'index2word of tailo : {output_lang.index2word}')\n","    with open(args.log, 'a', encoding='UTF-8') as f:\n","            f.write(f'>Building directory...\\n')\n","            f.write(f'word2index of chinese : {input_lang.word2index}\\n')\n","            f.write(f'index2word of tailo : {output_lang.index2word}\\n')\n","    return input_lang, output_lang, train_loader, valid_loader\n","\n","\n","\n","###########################################################################\n","##                            Model                                      ##\n","###########################################################################\n","class EncoderRNN(nn.Module):\n","    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n","        super(EncoderRNN, self).__init__()\n","        self.hidden_size = hidden_size\n","\n","        self.embedding = nn.Embedding(input_size, hidden_size)\n","        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n","        self.dropout = nn.Dropout(dropout_p)\n","\n","    def forward(self, input):\n","        embedded = self.dropout(self.embedding(input))\n","        output, hidden = self.gru(embedded)\n","        return output, hidden\n","\n","\n","class BahdanauAttention(nn.Module):\n","    def __init__(self, hidden_size):\n","        super(BahdanauAttention, self).__init__()\n","        self.Wa = nn.Linear(hidden_size, hidden_size)\n","        self.Ua = nn.Linear(hidden_size, hidden_size)\n","        self.Va = nn.Linear(hidden_size, 1)\n","\n","    def forward(self, query, keys):\n","        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n","        scores = scores.squeeze(2).unsqueeze(1)\n","\n","        weights = F.softmax(scores, dim=-1)\n","        context = torch.bmm(weights, keys)\n","\n","        return context, weights\n","\n","class AttnDecoderRNN(nn.Module):\n","    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n","        super(AttnDecoderRNN, self).__init__()\n","        self.embedding = nn.Embedding(output_size, hidden_size)\n","        self.attention = BahdanauAttention(hidden_size)\n","        self.gru = nn.GRU(2 * hidden_size, hidden_size, batch_first=True)\n","        self.out = nn.Linear(hidden_size, output_size)\n","        self.dropout = nn.Dropout(dropout_p)\n","\n","    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n","        batch_size = encoder_outputs.size(0)\n","        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n","\n","        decoder_hidden = encoder_hidden\n","        decoder_outputs = []\n","        attentions = []\n","\n","        for i in range(MAX_LENGTH):\n","            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n","                decoder_input, decoder_hidden, encoder_outputs\n","            )\n","            decoder_outputs.append(decoder_output)\n","            attentions.append(attn_weights)\n","\n","            if target_tensor is not None:\n","                # Teacher forcing: Feed the target as the next input\n","                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n","            else:\n","                # Without teacher forcing: use its own predictions as the next input\n","                _, topi = decoder_output.topk(1)\n","                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n","\n","        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n","        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n","        attentions = torch.cat(attentions, dim=1)\n","\n","        return decoder_outputs, decoder_hidden, attentions\n","\n","\n","    def forward_step(self, input, hidden, encoder_outputs):\n","        embedded =  self.dropout(self.embedding(input))\n","\n","        query = hidden.permute(1, 0, 2)\n","        context, attn_weights = self.attention(query, encoder_outputs)\n","        input_gru = torch.cat((embedded, context), dim=2)\n","\n","        output, hidden = self.gru(input_gru, hidden)\n","        output = self.out(output)\n","\n","        return output, hidden, attn_weights\n","\n","###########################################################################\n","##                           Training                                     ##\n","###########################################################################\n","\n","def train(encoder, decoder, train_loader, valid_loader, epochs, device):\n","    \"\"\"\n","    Args:\n","        encoder: encoder model\n","        decoder: decoder model\n","        train_loader: training dataloader\n","        valid_loader: validation dataloader\n","        epochs: number of training epoch\n","        device: gpu/cpu\n","    Returns:\n","        df_loss : dataframe with column 'epoch','loss_train','loss_valid'\n","        best_model_wts : saved model based on the best loss\n","    \"\"\"\n","    args = parse_config()\n","    df_loss=pd.DataFrame()\n","    df_loss['epoch']=range(1,epochs+1)\n","    best_model_wts=None\n","    best_evaluated_loss = 100000\n","\n","    encoder_optimizer = optim.Adam(encoder.parameters(), lr=args.lr)\n","    decoder_optimizer = optim.Adam(decoder.parameters(), lr=args.lr)\n","    loss_func = nn.CrossEntropyLoss()\n","\n","    acc_train=list()\n","    acc_valid=list()\n","    loss_train=list()\n","    loss_valid=list()\n","\n","    with open(args.log, 'a') as f:\n","        f.write(f'------------------------------------------\\n')\n","        f.write(f'-         Training       -\\n')\n","        f.write(f'------------------------------------------\\n')\n","        f.write(f'>>> Start training... \\n')\n","    for epoch in range(1,epochs+1):\n","        \"\"\"\n","        train\n","        \"\"\"\n","        with torch.set_grad_enabled(True):\n","\n","            t_loss=0\n","            t_acc=0\n","            correct=0\n","            for i, (data, labels) in enumerate(tqdm(train_loader, desc='Epoch '+str(epoch))):\n","\n","\n","                data, labels=data.to(device),labels.to(device)\n","                encoder_optimizer.zero_grad()\n","                decoder_optimizer.zero_grad()\n","                encoder_outputs, encoder_hidden = encoder(data)\n","                decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, labels)\n","\n","                topi = torch.argmax(decoder_outputs,dim=-1)\n","                decoded_ids = topi.squeeze()\n","\n","                loss = loss_func(\n","                    decoder_outputs.view(-1, decoder_outputs.size(-1)),\n","                    labels.view(-1)\n","                )\n","\n","                \"\"\"\n","                update\n","                \"\"\"\n","\n","                loss.backward()  # bp\n","                encoder_optimizer.step()\n","                decoder_optimizer.step()\n","                t_loss+=loss.item()\n","            t_loss/=len(train_loader.dataset)\n","            loss_train.append(t_loss)\n","\n","        \"\"\"\n","        evaluate\n","        \"\"\"\n","        # call function evaluate\n","        v_loss=evaluate(encoder, decoder, valid_loader, loss_func, device)\n","        loss_valid.append(v_loss)\n","        print(f'--Epoch{epoch:>2d} | Train loss:{t_loss:.6f} | Valid loss:{v_loss:.6f}\\n')\n","        with open(args.log, 'a') as f:\n","            f.write(f'--Epoch{epoch:>2d} | Train loss:{t_loss:.6f}| Valid loss:{v_loss:.6f}\\n')\n","\n","        # update best_model_wts\n","        if v_loss < best_evaluated_loss:\n","            best_evaluated_acc=v_loss\n","            best_model_wts={'encoder': encoder,\n","                            'decoder': decoder}\n","            torch.save(best_model_wts, os.path.join(args.outf, f'Best_Model.pt'))\n","\n","        # saved model\n","        checkpoint_dict = {'last_epoch': epoch,\n","                           'encoder': encoder.state_dict(),\n","                           'decoder': decoder.state_dict(),\n","                           'best_loss' : best_evaluated_loss,\n","                           'loss_train' : loss_train,\n","                           'loss_valid' : loss_valid}\n","        torch.save( checkpoint_dict, os.path.join(args.outf, f'Model.pt'))\n","\n","\n","    df_loss['loss_train']=loss_train\n","    df_loss['loss_valid']=loss_valid\n","\n","    return df_loss, best_model_wts\n","\n","def evaluate(encoder, decoder, valid_loader, loss_func, device):\n","    \"\"\"\n","    Args:\n","        encoder: encoder model\n","        decoder: decoder model\n","        valid_loader: validation dataloader\n","        loss_func : loss function\n","        device: gpu/cpu\n","    Returns:\n","        total_loss : validation average loss\n","    \"\"\"\n","\n","    args = parse_config()\n","\n","    print(f'valid:{len(valid_loader)}')\n","    with torch.set_grad_enabled(False):\n","\n","        correct=0\n","        total_loss=0\n","        acc=0\n","        for data, labels in valid_loader:\n","            data,labels=data.to(device),labels.to(device)\n","            encoder_outputs, encoder_hidden = encoder(data)\n","            decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, labels)\n","\n","            topi = torch.argmax(decoder_outputs,dim=-1)\n","            decoded_ids = topi.squeeze()\n","\n","            loss = loss_func(\n","                decoder_outputs.view(-1, decoder_outputs.size(-1)),\n","                labels.view(-1)\n","            )\n","\n","            total_loss+=loss.item()\n","\n","        total_loss/=len(valid_loader.dataset)\n","    return total_loss\n","\n","\n","\n","\n","###########################################################################\n","##                               Plot                                   ##\n","###########################################################################\n","def plot(dataframe1, mode):\n","    \"\"\"\n","    Arguments:\n","        dataframe1 : dataframe with 'epoch','loss_train','loss_valid' columns\n","        mode : loss/accuracy\n","    Returns:\n","        figure: figure of learning curve\n","    \"\"\"\n","    fig=plt.figure(figsize=(10,6))\n","    for name in dataframe1.columns[1:]:\n","        plt.plot(range(1,1+len(dataframe1)),name,data=dataframe1,label=name)\n","\n","    plt.xlabel('Epochs')\n","    if mode == 'loss' :\n","        plt.ylabel('Loss')\n","        plt.title('Learning curve (loss)')\n","    elif mode == 'acc':\n","        plt.ylabel('Accuracy(%)')\n","        plt.title('Learning curve (acc)')\n","    plt.legend()\n","    return fig\n","\n","###########################################################################\n","##                            Test                                       ##\n","###########################################################################\n","def test(input_lang, encoder, decoder,outf):\n","    \"\"\"\n","    Args:\n","        input_lang : directory of input language\n","        encoder: trained encoder model\n","        decoder: trained decoder model\n","        outf: path to output file\n","    \"\"\"\n","    df_id =list()\n","    df_txt = list()\n","    test_lines = pd.read_csv(args.test_data, usecols=[\"txt\"], encoding='utf-8').values.tolist()\n","    print(\"Read %s sentences of testing data\" % len(test_lines))\n","    with open(args.log, 'a') as f:\n","      f.write(f'------------------------------------------\\n')\n","      f.write(f'-        Testing        -\\n')\n","      f.write(f'------------------------------------------\\n')\n","      f.write(f'>> Testing data : {len(test_lines)}\\n')\n","    with torch.set_grad_enabled(False):\n","        encoder.eval()\n","        decoder.eval()\n","        correct=0\n","        i = 1\n","        for data in test_lines:\n","            input_tensor = tensorFromSentence(input_lang, data[0])\n","            encoder_outputs, encoder_hidden = encoder(input_tensor)\n","            decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n","\n","            topi = torch.argmax(decoder_outputs,dim=-1)\n","            decoded_ids = topi.squeeze()\n","            decoded_words = []\n","            for idx in decoded_ids:\n","                if idx.item() == EOS_token:\n","                    decoded_words.append('<EOS>')\n","                    break\n","                decoded_words.append(output_lang.index2word[idx.item()])\n","                output_sentence = ' '.join(decoded_words)\n","            df_id.append(i)\n","            df_txt.append(output_sentence)\n","            i=i+1\n","    for words in df_txt:\n","      if words == ' - ':\n","        words = '-'\n","    df = pd.DataFrame()\n","    df['id'] = df_id\n","    df['txt'] = df_txt\n","    df.to_csv(os.path.join(outf, 'test.csv'), index=None)\n","\n","###########################################################################\n","##                            Main                                      ##\n","###########################################################################\n","if __name__ == '__main__':\n","\n","\n","  if os.path.exists(args.log):\n","        os.remove(format(args.log))\n","\n","  with open(args.log, 'a') as f:\n","    f.write(f'------------------------------------------\\n')\n","    f.write(f'-      Initail settings      -\\n')\n","    f.write(f'------------------------------------------\\n')\n","    f.write(f'>> Epoch : {args.n_epochs}\\n')\n","    f.write(f'>> Batch size : {args.batch_size}\\n')\n","    f.write(f'>> Learning rate : {args.lr}\\n')\n","    f.write(f'>> Hidden size : {args.hidden_size}\\n')\n","    f.write(f'>> Optimizer : {args.opt}\\n')\n","    f.write(f'>> Loss func : {args.loss}\\n')\n","\n","  ## Data pre-processing\n","  input_lang, output_lang, train_loader, valid_loader = get_dataloader()\n","\n","  with open(args.log, 'a') as f:\n","    f.write(f'------------------------------------------\\n')\n","    f.write(f'-     Data Description      -\\n')\n","    f.write(f'------------------------------------------\\n')\n","    f.write(f'>> Trainig data : {len(train_loader.dataset)}\\n')\n","    f.write(f'>> Validation data : {len(valid_loader.dataset)}\\n')\n","\n","  ## Training\n","  encoder = EncoderRNN(input_lang.n_words, args.hidden_size).to(device)\n","  decoder = AttnDecoderRNN(args.hidden_size, output_lang.n_words).to(device)\n","  df_loss, best_model_wts = train(encoder, decoder, train_loader, valid_loader, args.n_epochs, device)\n","\n","  ## Learning curve\n","  fig_loss = plot(df_loss,'Learning curve')\n","  fig_loss.savefig(args.outf +  'Learning_curve' +'.png')\n","\n","  ## Testing\n","  test(input_lang, best_model_wts['encoder'], best_model_wts['decoder'], args.outf)\n","\n"],"metadata":{"id":"rh5eYxQxwVWN"},"execution_count":null,"outputs":[]}]}