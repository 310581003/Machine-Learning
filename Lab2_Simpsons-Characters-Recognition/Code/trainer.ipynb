{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyO97Hniaw+U8ASjpOG88fBN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"pjgbv8UkfDzj"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!7z x \"/content/drive/MyDrive/Colab Notebooks/2023ML/Lab2/train_agu_2.zip\" -o\"/content\""],"metadata":{"id":"VfN8Lk_HfG4e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!unzip '/content/drive/MyDrive/Colab Notebooks/2023ML/Lab2/test-final.zip' -d '/content'"],"metadata":{"id":"HESSHqHRfJQ7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install light-the-torch\n","!ltt install torch torchvision"],"metadata":{"id":"d86P4QLIfLT_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import sys\n","sys.path.append('/content/drive/MyDrive/Colab Notebooks/2023ML/Lab2')\n","print(torch.__version__)\n","print(torch.cuda.is_available())\n","\n","import os\n","from torch.utils.data import Dataset,DataLoader\n","import torchvision.transforms as T\n","from torchvision import models\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchvision.models.detection as detection\n","\n","import copy\n","\n","from PIL import Image\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import argparse\n","import csv\n","from pathlib import Path\n","from torch.utils.data import random_split\n","from tqdm import tqdm\n","\n","torch.backends.cudnn.benchmark = True\n","torch.cuda.get_device_name(0)"],"metadata":{"id":"tDoBhAvyfNaK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["###########################################################################\n","##          Argparse Setting                 ##\n","###########################################################################\n","def parse_config():\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument(\"--train_data_path\", default='/content/train_agu_2', help=\"1st-level folder of train data\")\n","    parser.add_argument(\"--train_overview\", default='/content/drive/MyDrive/Colab Notebooks/2023ML/Lab2/train_data_agu_2.csv', help=\"path of overview file for training data\")\n","    parser.add_argument(\"--test_data_path\", default='/content/test-final', help=\"folder of testing data\")\n","\n","    parser.add_argument(\"--n_epochs_1\", type=int, default=15, help=\"number of epochs of first-stage training\")\n","    parser.add_argument(\"--n_epochs_2\", type=int, default=30, help=\"number of epochs of second-stage training\")\n","    parser.add_argument(\"--num_class\", type=int, default=50, help=\"number of categories for classification\")\n","\n","    parser.add_argument(\"--batch_size\", type=int, default=200, help=\"size of the batches\")\n","    parser.add_argument(\"--num_workers\", type=int, default=4, help=\"num_workers for Dataloader\")\n","    parser.add_argument(\"--lr\", type=float, default=0.0035, help=\"adam: learning rate\")\n","    parser.add_argument(\"--opt\",  default='Adam', help=\"Optimizer:'SGD','RMSprop','Adagrad','Adam'\")\n","    parser.add_argument(\"--b1\", type=float, default=0.9, help=\"adam: decay of first order momentum of gradient\")\n","    parser.add_argument(\"--b2\", type=float, default=0.97, help=\"adam: decay of second order momentum of gradient\")\n","    parser.add_argument(\"--loss\", default='Cross_entropy', help=\"Loss func:'MAE','MSE','NLL(Negative Log-Likelihood)','Cross_entropy'\")\n","\n","    parser.add_argument('--outf', default='/content/drive/MyDrive/Colab Notebooks/2023ML/Lab2/Result_2', help='folder to output record and model')\n","    parser.add_argument('--log', default='/content/drive/MyDrive/Colab Notebooks/2023ML/Lab2/Result_2/record_2.txt', help='path to record')\n","    parser.add_argument('--font', default=20, help='font size for output figure')\n","    #parser.add_argument('--save_model', default='/content/drive/MyDrive/Colab Notebooks/2023ML/Lab2/Saved_Model', help='path to resume model weight')\n","    parser.add_argument('--resume', default='/content/drive/MyDrive/Colab Notebooks/2023ML/Lab2/Result_2', help='path to resume model weight')\n","    args = parser.parse_args(args=[])\n","    return args"],"metadata":{"id":"MNMQ-1ppfQBc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","###########################################################################\n","##             Categories                 ##\n","###########################################################################\n","Categories = {0  : \"abraham_grampa_simpson\",\n","              1  : \"agnes_skinner\",\n","              2  : \"apu_nahasapeemapetilon\",\n","              3  : \"barney_gumble\",\n","              4  : \"bart_simpson\",\n","              5  : \"brandine_spuckler\",\n","              6  : \"carl_carlson\",\n","              7  : \"charles_montgomery_burns\",\n","              8  : \"chief_wiggum\",\n","              9  : \"cletus_spuckler\",\n","              10 : \"comic_book_guy\",\n","              11 : \"disco_stu\",\n","              12 : \"dolph_starbeam\",\n","              13 : \"duff_man\",\n","              14 : \"edna_krabappel\",\n","              15 : \"fat_tony\",\n","              16 : \"gary_chalmers\",\n","              17 : \"gil\",\n","              18 : \"groundskeeper_willie\",\n","              19 : \"homer_simpson\",\n","              20 : \"jimbo_jones\",\n","              21 : \"kearney_zzyzwicz\",\n","              22 : \"kent_brockman\",\n","              23 : \"krusty_the_clown\",\n","              24 : \"lenny_leonard\",\n","              25 : \"lionel_hutz\",\n","              26 : \"lisa_simpson\",\n","              27 : \"lunchlady_doris\",\n","              28 : \"maggie_simpson\",\n","              29 : \"marge_simpson\",\n","              30 : \"martin_prince\",\n","              31 : \"mayor_quimby\",\n","              32 : \"milhouse_van_houten\",\n","              33 : \"miss_hoover\",\n","              34 : \"moe_szyslak\",\n","              35 : \"ned_flanders\",\n","              36 : \"nelson_muntz\",\n","              37 : \"otto_mann\",\n","              38 : \"patty_bouvier\",\n","              39 : \"principal_skinner\",\n","              40 : \"professor_john_frink\",\n","              41 : \"rainier_wolfcastle\",\n","              42 : \"ralph_wiggum\",\n","              43 : \"selma_bouvier\",\n","              44 : \"sideshow_bob\",\n","              45 : \"sideshow_mel\",\n","              46 : \"snake_jailbird\",\n","              47 : \"timothy_lovejoy\",\n","              48 : \"troy_mcclure\",\n","              49 : \"waylon_smithers\"}\n","\n","\n","###########################################################################\n","##           Preparing Data                 ##\n","###########################################################################\n","\n","#---------------------------- Dataloader ---------------------------#\n","class Simpsons_DataSet(Dataset):\n","    def __init__(self, mode):\n","\n","        args = parse_config()\n","\n","        self.img_path = args.train_data_path\n","        self.img_names = np.squeeze(pd.read_csv(args.train_overview, usecols=[\"name\"]).values)\n","        self.labels    = np.squeeze(pd.read_csv(args.train_overview, usecols=[\"label\"]).values)\n","        assert len(self.img_names)==len(self.labels), 'Error : Data and labels length not the same'\n","        self.data_len=len(self.img_names)\n","\n","        # Define the image augmentation transformations\n","        self.transformations = T.Compose([\n","                #T.ToPILImage(),  # Convert tensor back to PIL image for saving\n","                #T.AugMix(severity= 6,mixture_width=2),\n","                #T.RandomPosterize(bits=2, p=0.1),\n","                T.ToTensor(),\n","\n","                #T.RandomApply([T.RandomHorizontalFlip()], p=0.1),\n","                #T.RandomApply([T.RandomVerticalFlip()], p=0.1),\n","                #T.RandomApply([T.RandomRotation(10)], p=0.1),\n","\n","                #T.RandomApply([T.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1)], p=0.1),\n","                #T.RandomGrayscale(p=0.1),\n","                #T.RandomInvert(p=0.1),\n","\n","                #T.RandomApply([T.RandomSolarize(threshold=1.0)], p=0.05),\n","                #T.RandomApply([T.RandomAdjustSharpness(sharpness_factor=2)], p=0.1),\n","\n","                #T.RandomApply([AddGaussianNoise(0., 0.05)], p=0.1),  # mean and std\n","                #T.RandomApply([AddPoissonNoise(lam=0.1)], p=0.1),  # mean and std\n","                #T.RandomApply([AddSpeckleNoise(noise_level=0.1)], p=0.1),\n","                #T.RandomApply([AddSaltPepperNoise(salt_prob=0.05, pepper_prob=0.05)], p=0.1),\n","\n","                #T.RandomApply([T.RandomPerspective(distortion_scale=0.6, p=1.0)], p=0.1),\n","                #T.RandomApply([T.RandomAffine(degrees=(30, 70), translate=(0.1, 0.3), scale=(0.5, 0.75))], p=0.1),\n","                #T.RandomApply([T.ElasticTransform(alpha=250.0)], p=0.1),\n","\n","                #T.RandomApply([T.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5.))], p=0.1),\n","\n","                #T.RandomApply([AddGaussianNoise(0., 0.001)], p=1.0),  # mean and std\n","\n","                T.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n","\n","            ])\n","\n","\n","        print(f'>> Found {self.data_len} images for training ...')\n","        with open(args.log, 'a') as f:\n","            f.write(f'>> Found {self.data_len} images for training ...\\n')\n","            f.write(f'\\n')\n","    def __len__(self):\n","        return self.data_len\n","\n","    def __getitem__(self, index):\n","\n","\n","        single_img_name=os.path.join(self.img_path,self.img_names[index]+'.jpg')\n","\n","        single_img=Image.open(single_img_name).convert(\"RGB\")\n","        single_img = T.Resize((224, 224))(single_img)\n","        #single_img = np.asanyarray(single_img)\n","        #single_img = single_img.dtype(torch.uint8)\n","        #single_img = single_img.dtype(torch.uint8)\n","        img=self.transformations(single_img)\n","        label=self.labels[index]\n","        #print(f'labels : {label}')\n","\n","        return img, label\n","\n","###########################################################################\n","##              Model                   ##\n","###########################################################################\n","\n","class ResNet50(nn.Module):\n","    def __init__(self,num_class,pretrained=False):\n","        \"\"\"\n","        Args:\n","            num_class: #target class\n","            pretrained:\n","                True: the model will have pretrained weights, and only the last layer's 'requires_grad' is True(trainable)\n","                False: random initialize weights, and all layer's 'require_grad' is True\n","        \"\"\"\n","        super(ResNet50,self).__init__()\n","        self.model=models.efficientnet_v2_s(pretrained=pretrained)\n","        if pretrained:\n","            for param in self.model.parameters():\n","                param.requires_grad=False\n","\n","        num_neurons = self.model.classifier[1].in_features\n","        #num_neurons=self.model.fc.in_features\n","        #num_neurons = self.model.classifier[1].in_features\n","        self.model.classifier[1] = nn.Linear(num_neurons, num_class, bias=True)\n","        #self.model.fc=nn.Linear(num_neurons, num_class, bias=True)\n","\n","\n","    def forward(self,X):\n","        out=self.model(X)\n","        return out\n","\n","###########################################################################\n","##            Train & Validation              ##\n","###########################################################################\n","def train(model, train_loader, valid_loader, epochs, loss_func, optimizer, device, state):\n","    \"\"\"\n","    Args:\n","        model: resnet model\n","        train_loader: training dataloader\n","        valid_loader: validation dataloader\n","        loss_func: loss function\n","        optimizer: optimizer\n","        epochs: number of training epoch\n","        device: gpu/cpu\n","        num_class: #target class\n","        name: model name when saving model\n","    Returns:\n","        dataframe: with column 'epoch','acc_train','acc_test'\n","    \"\"\"\n","    args = parse_config()\n","    num_class = args.num_class\n","    df_loss=pd.DataFrame()\n","    df_acc=pd.DataFrame()\n","    df_loss['epoch']=range(1,epochs+1)\n","    df_acc['epoch']=range(1,epochs+1)\n","    best_model_wts=None\n","\n","    model.to(device)\n","\n","    if args.resume != '':\n","\n","        checkpoint = torch.load(os.path.join(args.resume, 'Model.pth'))\n","        model.load_state_dict(checkpoint['model_state_dict'])\n","        optimizer.load_state_dict(checkpoint['optim_state_dict'])\n","        #saved_param = torch.load(os.path.join(args.resume, 'param.pth'))\n","        best_evaluated_acc = checkpoint['best_acc']\n","        start_epoch =checkpoint['last_epoch']+1\n","        loss_train=checkpoint['loss_train']\n","        loss_valid=checkpoint['loss_valid']\n","        acc_train=checkpoint['acc_train']\n","        acc_valid=checkpoint['acc_valid']\n","\n","\n","    else:\n","      start_epoch = 1\n","      best_evaluated_acc = 0\n","      loss_train=list()\n","      loss_valid=list()\n","      acc_train=list()\n","      acc_valid=list()\n","\n","\n","    with open(args.log, 'a') as f:\n","        f.write(f'>>> Start training... \\n')\n","\n","    for epoch in range(start_epoch,epochs+1):\n","        \"\"\"\n","        train\n","        \"\"\"\n","        with torch.set_grad_enabled(True):\n","            model.train()\n","            t_loss=0\n","            correct=0\n","            for i, (img, label) in enumerate(tqdm(train_loader, desc='Epoch '+str(epoch))):\n","\n","                img, label=img.to(device),label.to(device,dtype=torch.long)\n","                predict=model(img)\n","                loss=loss_func(predict,label)\n","                t_loss+=loss.item()\n","                correct+=predict.max(dim=1)[1].eq(label).sum().item()\n","                \"\"\"\n","                update\n","                \"\"\"\n","                optimizer.zero_grad()\n","                loss.backward()  # bp\n","                optimizer.step()\n","            t_loss/=len(train_loader.dataset)\n","            t_acc=100.*correct/len(train_loader.dataset)\n","            loss_train.append(t_loss)\n","            acc_train.append(t_acc)\n","\n","        \"\"\"\n","        evaluate\n","        \"\"\"\n","        _, v_loss, v_acc=evaluate(model, valid_loader, loss_func, device)\n","        loss_valid.append(v_loss)\n","        acc_valid.append(v_acc)\n","        print(f'epoch{epoch:>2d} | Train loss:{t_loss:.4f} | Train acc:{t_acc:.2f} | Valid loss:{v_loss:.4f} | Valid acc:{v_acc:.2f}\\n')\n","        with open(args.log, 'a') as f:\n","            f.write(f'epoch{epoch:>2d} | Train loss:{t_loss:.4f} | Train acc:{t_acc:.2f} | Valid loss:{v_loss:.4f} | Valid acc:{v_acc:.2f}\\n')\n","\n","        # update best_model_wts\n","        if v_acc > best_evaluated_acc:\n","            best_evaluated_acc=v_acc\n","            best_model_wts=copy.deepcopy(model)\n","            torch.save(model, os.path.join(args.outf, 'Best_Model.pth'))\n","\n","        checkpoint_dict = {'last_epoch': epoch,\n","                           'model_state_dict': model.state_dict(),\n","                           'optim_state_dict': optimizer.state_dict(),\n","                           'best_acc' : best_evaluated_acc,\n","                           'loss_train' : loss_train,\n","                           'loss_valid' : loss_valid,\n","                           'acc_train' : acc_train,\n","                           'acc_valid' : acc_valid,\n","                           'state' : state}\n","        torch.save( checkpoint_dict, os.path.join(args.outf, 'Model.pth'))\n","\n","\n","    df_loss['loss_train']=loss_train\n","    df_loss['loss_valid']=loss_valid\n","    df_acc['acc_train']=acc_train\n","    df_acc['acc_valid']=acc_valid\n","\n","    # save model\n","    #torch.save(best_model_wts,os.path.join(args.save_model+'.pt'))\n","    #model.load_state_dict(best_model_wts)\n","\n","    return df_loss, df_acc, best_model_wts\n","\n","def evaluate(model, valid_loader, loss_func, device):\n","    \"\"\"\n","    Args:\n","        model: resnet model\n","        valid_loader: validation dataloader\n","        device: gpu/cpu\n","    Returns:\n","        confusion_matrix: (num_class,num_class) ndarray\n","        total_loss : validation average loss\n","        acc: validation accuracy\n","    \"\"\"\n","\n","    args = parse_config()\n","    num_class = args.num_class\n","    confusion_matrix=np.zeros((num_class,num_class))\n","\n","    with torch.set_grad_enabled(False):\n","        model.eval()\n","        correct=0\n","        total_loss=0\n","        for img, label in valid_loader:\n","            img,label=img.to(device),label.to(device,dtype=torch.long)\n","            predict=model(img)\n","            predict_class=predict.max(dim=1)[1]\n","            loss=loss_func(predict,label)\n","            total_loss+=loss.item()\n","            correct+=predict_class.eq(label).sum().item()\n","            for i in range(len(label)):\n","                confusion_matrix[int(label[i])][int(predict_class[i])]+=1\n","        total_loss/=len(valid_loader.dataset)\n","        acc=100.*correct/len(valid_loader.dataset)\n","\n","    # normalize confusion_matrix\n","    confusion_matrix=confusion_matrix/confusion_matrix.sum(axis=1).reshape(num_class,1)\n","\n","    return confusion_matrix, total_loss,acc\n","\n","###########################################################################\n","##              Plot                   ##\n","###########################################################################\n","def plot(dataframe1, mode):\n","    \"\"\"\n","    Arguments:\n","        dataframe1: dataframe with 'epoch','loss_train','loss_valid','acc_train','acc_valid' columns of without pretrained weights model\n","        title: figure's title\n","    Returns:\n","        figure: an figure\n","    \"\"\"\n","    fig=plt.figure(figsize=(10,6))\n","    for name in dataframe1.columns[1:]:\n","        plt.plot(range(1,1+len(dataframe1)),name,data=dataframe1,label=name)\n","\n","    plt.xlabel('Epochs')\n","    if mode == 'loss' :\n","        plt.ylabel('Loss')\n","        plt.title('Learning curve (loss)')\n","    elif mode == 'acc':\n","        plt.ylabel('Accuracy(%)')\n","        plt.title('Learning curve (acc)')\n","    plt.legend()\n","    return fig\n","\n","def plot_confusion_matrix(confusion_matrix):\n","    args = parse_config()\n","    label_name = [\"abraham_grampa_simpson\", \"agnes_skinner\", \"apu_nahasapeemapetilon\", \"barney_gumble\", \"bart_simpson\",\n","                  \"brandine_spuckler\", \"carl_carlson\", \"charles_montgomery_burns\", \"chief_wiggum\", \"cletus_spuckler\",\n","                  \"comic_book_guy\", \"disco_stu\", \"dolph_starbeam\", \"duff_man\", \"edna_krabappel\",\n","                  \"fat_tony\", \"gary_chalmers\", \"gil\", \"groundskeeper_willie\", \"homer_simpson\",\n","                  \"jimbo_jones\", \"kearney_zzyzwicz\", \"kent_brockman\", \"krusty_the_clown\", \"lenny_leonard\",\n","                  \"lionel_hutz\", \"lisa_simpson\", \"lunchlady_doris\", \"maggie_simpson\", \"marge_simpson\",\n","                  \"martin_prince\", \"mayor_quimby\", \"milhouse_van_houten\", \"miss_hoover\", \"moe_szyslak\",\n","                  \"ned_flanders\", \"nelson_muntz\", \"otto_mann\", \"patty_bouvier\", \"principal_skinner\",\n","                  \"professor_john_frink\", \"rainier_wolfcastle\", \"ralph_wiggum\", \"selma_bouvier\", \"sideshow_bob\",\n","                  \"sideshow_mel\", \"snake_jailbird\", \"timothy_lovejoy\", \"troy_mcclure\", \"waylon_smithers\"]\n","\n","    fig, ax = plt.subplots(figsize=(25,25))\n","    ax.matshow(confusion_matrix, cmap=plt.cm.Blues)\n","    ax.xaxis.set_label_position('top')\n","\n","    for i in range(confusion_matrix.shape[0]):\n","        for j in range(confusion_matrix.shape[1]):\n","            ax.text(i, j, '{:.2f}'.format(confusion_matrix[j, i]), va='center', ha='center')\n","    ax.set(xticks=range(0, 50, 1), xticklabels=label_name, yticks=range(0, 50,1), yticklabels=label_name)\n","    ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n","\n","    ax.set_xlabel('Predicted label', fontsize = 15)\n","    ax.set_ylabel('True label', fontsize = 15)\n","    return fig\n","\n","###########################################################################\n","##              Test                   ##\n","###########################################################################\n","def test(model, outf):\n","    \"\"\"\n","    Args:\n","        model: trained model\n","        testing_dataset: testing dataset\n","        outf: path to output file\n","    \"\"\"\n","    args = parse_config()\n","    df_id =list()\n","    df_character = list()\n","    img_path = args.test_data_path\n","    # Define the image augmentation transformations\n","    transformations = T.Compose([\n","        T.ToTensor(),  # Convert tensor back to PIL image for saving\n","        T.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225]),\n","    ])\n","    img_names =list()\n","    count = 0\n","    with torch.set_grad_enabled(False):\n","        model.eval()\n","        correct=0\n","        i = 1\n","        for filename in os.listdir(img_path):\n","            count=count+1\n","            file_name=Path(filename).stem\n","            img_names.append(file_name)\n","            single_img_name=os.path.join(img_path,file_name+'.jpg')\n","            single_img=Image.open(single_img_name).convert(\"RGB\")\n","            single_img = T.Resize((224, 224))(single_img)\n","            img=transformations(single_img)\n","            img=img.to(device)\n","            predict=model(img.unsqueeze(0))\n","            df_id.append(file_name)\n","            print(torch.argmax(predict,1))\n","            df_character.append(Categories.get(int(torch.argmax(predict,1))))\n","            i=i+1\n","\n","    data_len = count\n","    df = pd.DataFrame()\n","\n","    df['id'] = df_id\n","    df['character'] = df_character\n","    df.to_csv(os.path.join(outf, 'test.csv'))\n","\n","    return data_len, df\n","\n","###########################################################################\n","##              Main                    ##\n","###########################################################################\n","\n","if __name__ == '__main__':\n","\n","    args = parse_config()\n","    if args.resume == '':\n","      if os.path.exists(args.log):\n","        os.remove(format(args.log))\n","\n","\n","      with open(args.log, 'a') as f:\n","          f.write(f'----------------------------------------------------------\\n')\n","          f.write(f'-                    Hyperparameters                     -\\n')\n","          f.write(f'----------------------------------------------------------\\n')\n","          f.write(f'>> Epoch :                  first stage :{args.n_epochs_1} | second stage :{args.n_epochs_2}\\n')\n","          f.write(f'>> Optimizer :              {args.opt}\\n')\n","          f.write(f'>> Learning rate :          {args.lr}\\n')\n","          f.write(f'>> Loss func :              {args.loss}\\n')\n","          f.write(f'>> Batch size :             {args.batch_size}\\n')\n","\n","\n","      with open(args.log, 'a') as f:\n","          f.write(f'----------------------------------------------------------\\n')\n","          f.write(f'-                    Training......                      -\\n')\n","          f.write(f'----------------------------------------------------------\\n')\n","\n","    \"\"\"\n","        Data Loading\n","    \"\"\"\n","    #---- Train & Valid data loading ----#\n","    train_dataset = Simpsons_DataSet(mode='train')\n","    ## Train & valid split\n","    train_size = int(0.8 * len(train_dataset))\n","    valid_size = len(train_dataset) - train_size\n","    train_dataset, valid_dataset = random_split(train_dataset, [train_size, valid_size])\n","    ## Train & valid dataloader\n","    train_loader=DataLoader(dataset=train_dataset, batch_size=args.batch_size, shuffle=True, num_workers = args.num_workers)\n","    valid_loader=DataLoader(dataset=valid_dataset, batch_size=args.batch_size, shuffle=True, num_workers = args.num_workers)\n","\n","\n","    \"\"\"\n","        Train\n","    \"\"\"\n","    print(\">>> Start training...\\n\")\n","    ## Device\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","    ## Loss Func\n","    if args.loss == 'MAE' :\n","        loss_func = nn.L1Loss()\n","    elif args.loss == 'MSE' :\n","        loss_func = nn.MSELoss()\n","    elif args.loss == 'NLL' :\n","        loss_func = nn.NLLLoss()\n","    elif args.loss == 'Cross_entropy' :\n","        loss_func = nn.CrossEntropyLoss()\n","    ## Train calling\n","    \"\"\"\n","    resnet50 with pretrained weights\n","        first feature extraction for few epochs, then finefuning for some epochs\n","    \"\"\"\n","\n","    model_with=ResNet50(num_class=args.num_class, pretrained=True)\n","\n","    if args.resume != '':\n","        checkpoint = torch.load(os.path.join(args.resume, 'Model.pth'))\n","        best_evaluated_acc = checkpoint['best_acc']\n","        state =checkpoint['state']\n","    else :\n","        state = 0\n","\n","\n","    if state == 0 :\n","\n","        # feature extraction\n","        print(f'>>> Feature extraction...\\n')\n","        with open(args.log, 'a') as f:\n","            f.write(f'>>> Feature extraction...\\n')\n","\n","        params_to_update=[]\n","        for name,param in model_with.named_parameters():\n","            if param.requires_grad:\n","                params_to_update.append(param)\n","        optimizer=optim.Adam(params_to_update, lr=args.lr, betas=(args.b1, args.b2), eps=1e-08, weight_decay=2e-5, amsgrad=True)\n","        df_loss_1, df_acc_1, best_model_wts_1=train(model_with, train_loader, valid_loader, args.n_epochs_1, loss_func,optimizer, device, state)\n","        state = 1\n","        torch.save({'df_loss_1' : df_loss_1,\n","                    'df_acc_1' : df_acc_1},\n","                    '%s/loss_acc.pth' % args.outf)\n","\n","        # finetuning\n","        print(f'>>> Fine-tuning...\\n')\n","        with open(args.log, 'a') as f:\n","            f.write(f'>>> Fine-tuning...\\n')\n","        for param in model_with.parameters():\n","            param.requires_grad=True\n","        optimizer=optim.Adam(model_with.parameters(), lr=args.lr, betas=(args.b1, args.b2), eps=1e-08, weight_decay=2e-5, amsgrad=True)\n","        df_loss_2, df_acc_2, best_model_wts_2=train(model_with, train_loader, valid_loader, args.n_epochs_2, loss_func,optimizer, device, state)\n","        state = 2\n","\n","    elif state == 1 :\n","        # finetuning\n","        saved_loss_acc = torch.load(os.path.join(args.resume, 'loss_acc.pth'))\n","        df_loss_1 = saved_loss_acc['df_loss_1']\n","        df_acc_1 = saved_loss_acc['df_acc_1']\n","        print(f'>>> Fine-tuning...\\n')\n","        #with open(args.log, 'a') as f:\n","            #f.write(f'>>> Fine-tuning...\\n')\n","        for param in model_with.parameters():\n","            param.requires_grad=True\n","        optimizer=optim.Adam(model_with.parameters(), lr=args.lr, betas=(args.b1, args.b2), eps=1e-08, weight_decay=2e-5, amsgrad=True)\n","        df_loss_2, df_acc_2, best_model_wts_2=train(model_with, train_loader, valid_loader, args.n_epochs_2, loss_func,optimizer, device, state)\n","\n","        state = 2\n","\n","\n","    df_loss = pd.concat([df_loss_1,df_loss_2],axis=0,ignore_index=True)\n","    df_acc = pd.concat([df_acc_1,df_acc_2],axis=0,ignore_index=True)\n","\n","    \"\"\"\n","        plot & save\n","    \"\"\"\n","\n","    #---- Plot confusion matrix ----#\n","    confusion_matrix, _, __ = evaluate (torch.load(os.path.join(args.outf, 'Best_Model.pth')), valid_loader, loss_func, device)\n","    figure=plot_confusion_matrix(confusion_matrix)\n","    figure.savefig(os.path.join(args.outf, 'Confusion matrix.png'))\n","\n","    #---- Plot learning curve ----#\n","    figure=plot(df_loss,'loss')\n","    figure.savefig(os.path.join(args.outf, 'Learning curve (loss).png'))\n","\n","    figure=plot(df_acc,'acc')\n","    figure.savefig(os.path.join(args.outf, 'Learning curve (acc).png'))\n","\n","    #---- Output predictions of testing data ----#\n","    with open(args.log, 'a') as f:\n","        f.write(f'----------------------------------------------------------\\n')\n","        f.write(f'-                     Testing......                      -\\n')\n","        f.write(f'----------------------------------------------------------\\n')\n","    test_len, prediction = test(torch.load(os.path.join(args.outf, 'Best_Model.pth')), args.outf)\n","    with open(args.log, 'a') as f:\n","        f.write(f'>> Found {test_len} images for testning ...\\n')\n","        f.write(f'\\n')\n","        f.write(f'>> Prediction :\\n')\n","        f.write(f'{prediction}')\n"],"metadata":{"id":"UbGx2Rw7fTcL"},"execution_count":null,"outputs":[]}]}